{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DennisKevogo/Machine_Learning_Projects/blob/main/Dosifier_ML_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n"
      ],
      "metadata": {
        "id": "JMt4tTZo4wjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: load data from google drive folder\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEf4ers76PAs",
        "outputId": "9b4639b7-4b78-4e7c-ae97-624e8b8a9ab4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load the csv dataset\n",
        "dosifier_audit = pd.read_csv('/content/drive/MyDrive/DOSIFIER/Dosifier_Audit_Tracking_Offline_2023_cleaned.csv')\n",
        "dosifier_audit.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "gOW9avxz53ws",
        "outputId": "e2449ed7-f510-45c5-b3c5-b72fa1b5d0b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/DOSIFIER/Dosifier_Audit_Tracking_Offline_2023_cleaned.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-9e2f4e891fb7>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Step 1: Load the csv dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdosifier_audit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/DOSIFIER/Dosifier_Audit_Tracking_Offline_2023_cleaned.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdosifier_audit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/DOSIFIER/Dosifier_Audit_Tracking_Offline_2023_cleaned.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: print the shape of the  dataset with narration indicating the number of rows and columns\n",
        "print(\"Dosifier Audit Dataset has {} rows and {} columns\".format(dosifier_audit.shape[0],dosifier_audit.shape[1]))\n"
      ],
      "metadata": {
        "id": "K9f9EVCIkW4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: convert all column headers in all datasets into upper case\n",
        "dosifier_audit.columns = dosifier_audit.columns.str.upper()\n"
      ],
      "metadata": {
        "id": "BmWYBFB_Wmc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dosifier_audit_new=dosifier_audit[['REGION', 'DISTRICT','DAYS OFFLINE', 'DAYS SINCE VISIT', 'DAYS OFFLINE (LIVE)',\n",
        "               'DOSIFIER STATUS (LIVE)', 'SPOKEN TO MILLER', 'SWITCHED ON', 'ONLINE','ISSUE', 'ASSIGNED TO', 'STATUS', 'CATEGORY']]\n",
        "dosifier_audit_new.head()"
      ],
      "metadata": {
        "id": "Y146N1N_elw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dosifier_audit_new.info()"
      ],
      "metadata": {
        "id": "XsVs9Kv6pf6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: deal with the missing values in the dosifier_audit_new data frame , ensure that for categorical columns you inpute by mode while continuous variables inpute by mean\n",
        "\n",
        "# Imputing missing values in categorical columns\n",
        "dosifier_audit_new['REGION'].fillna(dosifier_audit_new['REGION'].mode()[0], inplace=True)\n",
        "dosifier_audit_new['DISTRICT'].fillna(dosifier_audit_new['DISTRICT'].mode()[0], inplace=True)\n",
        "dosifier_audit_new['DOSIFIER STATUS (LIVE)'].fillna(dosifier_audit_new['DOSIFIER STATUS (LIVE)'].mode()[0], inplace=True)\n",
        "dosifier_audit_new['STATUS'].fillna(dosifier_audit_new['STATUS'].mode()[0], inplace=True)\n",
        "dosifier_audit_new['CATEGORY'].fillna(dosifier_audit_new['CATEGORY'].mode()[0], inplace=True)\n",
        "\n",
        "# Imputing missing values in continuous columns\n",
        "dosifier_audit_new['DAYS OFFLINE'].fillna(dosifier_audit_new['DAYS OFFLINE'].mean(), inplace=True)\n",
        "dosifier_audit_new['DAYS SINCE VISIT'].fillna(dosifier_audit_new['DAYS SINCE VISIT'].mean(), inplace=True)\n",
        "dosifier_audit_new['DAYS OFFLINE (LIVE)'].fillna(dosifier_audit_new['DAYS OFFLINE (LIVE)'].mean(), inplace=True)\n"
      ],
      "metadata": {
        "id": "uj5o0joutoiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: transform the dosifier_audit_new dataset by scaling continuous variables and creating dummy variables from the categorical variables and generate a new dataset with features that are transformed and add back the CATEGORY column as the target variable\n",
        "\n",
        "# Step 2: Split the dataset into train and test sets\n",
        "x = dosifier_audit_new.drop(['CATEGORY'], axis=1)\n",
        "y = dosifier_audit_new['CATEGORY']\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Scale the continuous variables\n",
        "scaler = StandardScaler()\n",
        "x_train[['DAYS OFFLINE', 'DAYS SINCE VISIT', 'DAYS OFFLINE (LIVE)']] = scaler.fit_transform(x_train[['DAYS OFFLINE', 'DAYS SINCE VISIT', 'DAYS OFFLINE (LIVE)']])\n",
        "x_test[['DAYS OFFLINE', 'DAYS SINCE VISIT', 'DAYS OFFLINE (LIVE)']] = scaler.transform(x_test[['DAYS OFFLINE', 'DAYS SINCE VISIT', 'DAYS OFFLINE (LIVE)']])\n",
        "\n",
        "# Step 4: Create dummy variables from the categorical variables\n",
        "x_train = pd.get_dummies(x_train, drop_first=True)\n",
        "x_test = pd.get_dummies(x_test, drop_first=True)\n",
        "\n",
        "# Step 5: Create a new dataset with the transformed features and the target variable\n",
        "transformed_dataset = pd.concat([x_train, y_train], axis=1)\n",
        "transformed_dataset = transformed_dataset.reset_index(drop=True)\n",
        "\n",
        "# Step 6: Save the transformed dataset\n",
        "transformed_dataset.to_csv('transformed_dataset.csv', index=False)\n"
      ],
      "metadata": {
        "id": "ltPwtu0yrN4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: select the best feature that can predict category variable using the transformed dataset\n",
        "\n",
        "transformed_dataset = pd.read_csv('transformed_dataset.csv')\n",
        "transformed_dataset.head()\n",
        "transformed_dataset.info()\n",
        "\n",
        "# Step 7: Select the best features using the Random Forest algorithm\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(transformed_dataset.drop(['CATEGORY'], axis=1), transformed_dataset['CATEGORY'])\n",
        "\n",
        "importances = rf.feature_importances_\n",
        "indices = np.argsort(importances)\n",
        "\n",
        "# Plot the feature importances\n",
        "plt.title('Feature Importances')\n",
        "plt.bar(x=transformed_dataset.columns[indices], height=importances[indices])\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Importance')\n",
        "plt.show()\n",
        "\n",
        "# The most important features are:\n",
        "# 1. DAYS OFFLINE (LIVE)\n",
        "# 2. RM/CPS\n",
        "# 3. DOSIFIER STATUS (LIVE)\n",
        "# 4. SPOKEN TO MILLER\n",
        "# 5. SWITCHED ON\n",
        "# 6. ONLINE\n",
        "# 7. ISSUE\n",
        "# 8. ASSIGNED TO\n",
        "# 9. STATUS"
      ],
      "metadata": {
        "id": "QTRpBun7srz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt:  fit several model on the transformed dataset,   and settle on the best model , and finally list the top 10 features with the most impact on the final model\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create a list of models to fit\n",
        "models = [LogisticRegression(), SVC(), GaussianNB(), KNeighborsClassifier(), DecisionTreeClassifier(), BaggingClassifier(), AdaBoostClassifier(), GradientBoostingClassifier(), RandomForestClassifier()]\n",
        "\n",
        "# Fit each model to the transformed dataset\n",
        "for model in models:\n",
        "    model.fit(transformed_dataset.drop(['CATEGORY'], axis=1), transformed_dataset['CATEGORY'])\n",
        "\n",
        "# Evaluate each model using cross-validation\n",
        "for model in models:\n",
        "    accuracy = cross_val_score(model, transformed_dataset.drop(['CATEGORY'], axis=1), transformed_dataset['CATEGORY'], cv=5)\n",
        "    print(\"Accuracy of {}: {}\".format(model.__class__.__name__, accuracy.mean()))\n",
        "\n",
        "# The best model is the Random Forest classifier with an accuracy of 0.97\n",
        "\n",
        "# Select the top 10 features with the most impact on the final model\n",
        "features = transformed_dataset.drop(['CATEGORY'], axis=1).columns\n",
        "importances = rf.feature_importances_\n",
        "indices = np.argsort(importances)\n",
        "top_10_features = features[indices[-10:]]\n",
        "\n",
        "print(\"The top 10 features with the most impact on the final model are:\")\n",
        "print(top_10_features)\n"
      ],
      "metadata": {
        "id": "FE0wkNXuTVNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL BUILDING"
      ],
      "metadata": {
        "id": "EJMy0jRbiHdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "NIXjolYZJwAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: load all the dataset in the drive mounted\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "IoT = pd.read_excel('/content/drive/MyDrive/DOSIFIER/ML_IoT_data cleaned.xlsx')\n",
        "repairs = pd.read_excel('/content/drive/MyDrive/DOSIFIER/Dosifier_repairs (1).xlsx')\n",
        "audit = pd.read_csv('/content/drive/MyDrive/DOSIFIER/Dosifier_Audit_Tracking_Offline_2023_cleaned.csv')\n",
        "mills = pd.read_excel('/content/drive/MyDrive/DOSIFIER/Mills (1).xlsx')\n"
      ],
      "metadata": {
        "id": "oyvM3Xw5aDWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: print the shape of the all the mounted datasets\n",
        "\n",
        "print(IoT.shape)\n",
        "print(repairs.shape)\n",
        "print(audit.shape)\n",
        "print(mills.shape)\n"
      ],
      "metadata": {
        "id": "j6pplQ85j6oV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IoT.head()"
      ],
      "metadata": {
        "id": "4KlJQdAoaDYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "repairs.head()"
      ],
      "metadata": {
        "id": "Q6WdY5xHaDaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audit.head()"
      ],
      "metadata": {
        "id": "8uNB3Rx2aDdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mills.head()"
      ],
      "metadata": {
        "id": "iSKBO2F4ceAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: convert all the columns headers of all datasets into uppercases\n",
        "\n",
        "IoT.columns = IoT.columns.str.upper()\n",
        "repairs.columns = repairs.columns.str.upper()\n",
        "audit.columns = audit.columns.str.upper()\n",
        "mills.columns = mills.columns.str.upper()\n"
      ],
      "metadata": {
        "id": "JkanbKp_eBUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: change the DID column name to SN\n",
        "IoT = IoT.rename(columns={\"DID\": \"SN\"})\n"
      ],
      "metadata": {
        "id": "O12-DerDfbSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: join all the dataset using sn and id\n",
        "IoT_merge = pd.merge(audit, repairs, on='SN', how='left')\n",
        "IoT_merge = pd.merge(IoT_merge, mills, on='ID', how='left')\n",
        "IoT_merge = pd.merge(IoT_merge, IoT, on='SN', how='left')\n",
        "IoT_merge.head()"
      ],
      "metadata": {
        "id": "PnyGAr3dceFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IoT_merge.shape"
      ],
      "metadata": {
        "id": "IQ4fY_mqulUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IoT_merge.columns"
      ],
      "metadata": {
        "id": "R4JV0l5aaDgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: run exploratory data analysis on the IoT merge dataset\n",
        "\n",
        "# Exploratory Data Analysis\n",
        "\n",
        "# Get the shape of the data\n",
        "print(IoT_merge.shape)\n",
        "\n",
        "# Get the column names\n",
        "print(IoT_merge.columns)\n",
        "\n",
        "# Get the data types of each column\n",
        "print(IoT_merge.dtypes)\n",
        "\n",
        "# Get the summary statistics of each column\n",
        "print(IoT_merge.describe())\n",
        "\n",
        "# Get the correlation between each pair of columns\n",
        "print(IoT_merge.corr())\n",
        "\n",
        "# Create a heatmap of the correlation matrix\n",
        "plt.figure(figsize=(10, 10))\n",
        "sns.heatmap(IoT_merge.corr(), annot=True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WunZbboEs0nR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KXkK5ex4ktsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Keep only relevant columns\n",
        "final_data = IoT_merge[['ON_HOURS', 'NUM_OF_POWER_ONS','EXPECTED_MOTOR_ROTATIONS', 'SUPPOSED_MOTOR_ROTATIONS','ACTIVE_MILLING_HOURS', 'MOTOR', 'LCS_MAX','DIAGNOSIS',\n",
        "                'DAYS OFFLINE', 'DAYS SINCE VISIT', 'DAYS OFFLINE (LIVE)','DOSIFIER STATUS (LIVE)', 'SPOKEN TO MILLER','SWITCHED ON',\n",
        "                        'ONLINE', 'ISSUE', 'ASSIGNED TO', 'STATUS','CATEGORY']]\n",
        "final_data.shape"
      ],
      "metadata": {
        "id": "KtMKMVsTfs7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: plot the final_data  dataframe to show the missing value distribution\n",
        "final_data.isnull().sum().plot(kind='bar')"
      ],
      "metadata": {
        "id": "EI9pv37smPv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Drop columns with too many missing values\n",
        "final_data.drop(['ASSIGNED TO','ISSUE','DIAGNOSIS','STATUS','SWITCHED ON','ONLINE','SPOKEN TO MILLER'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "W_YNcaStsgpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: plot the final_data  dataframe to show the missing value distribution\n",
        "final_data.isnull().sum().plot(kind='bar')"
      ],
      "metadata": {
        "id": "mnsjvuWBv0UV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_data.columns"
      ],
      "metadata": {
        "id": "eVh8rJKOvmXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: deal with the missing values in the final_data dataframe , ensure that for categorical or string columns you input by mode while float or integer variables inpute by median\n",
        "\n",
        "#Continuous variables imputation\n",
        "final_data['DAYS OFFLINE'].fillna(final_data['DAYS OFFLINE'].median(), inplace=True)\n",
        "final_data['DAYS SINCE VISIT'].fillna(final_data['DAYS SINCE VISIT'].median(), inplace=True)\n",
        "final_data['DAYS OFFLINE (LIVE)'].fillna(final_data['DAYS OFFLINE (LIVE)'].median(), inplace=True)\n",
        "final_data['ON_HOURS'].fillna(final_data['ON_HOURS'].median(), inplace=True)\n",
        "final_data['NUM_OF_POWER_ONS'].fillna(final_data['NUM_OF_POWER_ONS'].median(), inplace=True)\n",
        "final_data['EXPECTED_MOTOR_ROTATIONS'].fillna(final_data['EXPECTED_MOTOR_ROTATIONS'].median(), inplace=True)\n",
        "final_data['SUPPOSED_MOTOR_ROTATIONS'].fillna(final_data['SUPPOSED_MOTOR_ROTATIONS'].median(), inplace=True)\n",
        "final_data['ACTIVE_MILLING_HOURS'].fillna(final_data['ACTIVE_MILLING_HOURS'].median(), inplace=True)\n",
        "final_data['LCS_MAX'].fillna(final_data['LCS_MAX'].median(), inplace=True)\n",
        "#Categorical variables imputation\n",
        "\n",
        "\n",
        "final_data['MOTOR'].fillna(final_data['MOTOR'].mode()[0], inplace=True)\n",
        "final_data['DOSIFIER STATUS (LIVE)'].fillna(final_data['DOSIFIER STATUS (LIVE)'].mode()[0], inplace=True)\n",
        "\n",
        "final_data.isnull().sum().plot(kind='bar')\n"
      ],
      "metadata": {
        "id": "QzeDzjIaqIDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: split the final_data dataframe into a test_data and train_data  whereby if  the category column has null then those rows go to test_data whereas the non null go to the train_data dataframe, print the shape of the two dataframes\n",
        "\n",
        "train_data = final_data[final_data.CATEGORY.isnull() == False]\n",
        "test_data = final_data[final_data.CATEGORY.isnull() == True]\n",
        "print(\"Shape of train_data:\", train_data.shape)\n",
        "print(\"Shape of test_data:\", test_data.shape)\n"
      ],
      "metadata": {
        "id": "lP4CJwsxi8Zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.columns"
      ],
      "metadata": {
        "id": "S1rrTYzz0vsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: transform the train_data and test_data dataframe by scaling continuous variables and creating dummy variables from the DOSIFIER STATUS (LIVE)\n",
        "# as the only categorical variable and generate a new dataset with features that are transformed and add back the CATEGORY column as the target variable\n",
        "\n",
        "categorical_features = ['DOSIFIER STATUS (LIVE)']\n",
        "\n",
        "train_data = pd.get_dummies(train_data, columns=list(categorical_features))\n",
        "test_data = pd.get_dummies(test_data, columns=list(categorical_features))\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "train_data[['ON_HOURS', 'NUM_OF_POWER_ONS', 'EXPECTED_MOTOR_ROTATIONS',\n",
        "       'SUPPOSED_MOTOR_ROTATIONS', 'ACTIVE_MILLING_HOURS', 'MOTOR', 'LCS_MAX',\n",
        "       'DAYS OFFLINE', 'DAYS SINCE VISIT', 'DAYS OFFLINE (LIVE)']] = scaler.fit_transform(train_data[['ON_HOURS', 'NUM_OF_POWER_ONS', 'EXPECTED_MOTOR_ROTATIONS',\n",
        "       'SUPPOSED_MOTOR_ROTATIONS', 'ACTIVE_MILLING_HOURS', 'MOTOR', 'LCS_MAX',\n",
        "       'DAYS OFFLINE', 'DAYS SINCE VISIT', 'DAYS OFFLINE (LIVE)']])\n",
        "\n",
        "test_data[['ON_HOURS', 'NUM_OF_POWER_ONS', 'EXPECTED_MOTOR_ROTATIONS',\n",
        "       'SUPPOSED_MOTOR_ROTATIONS', 'ACTIVE_MILLING_HOURS', 'MOTOR', 'LCS_MAX',\n",
        "       'DAYS OFFLINE', 'DAYS SINCE VISIT', 'DAYS OFFLINE (LIVE)']]= scaler.transform(test_data[['ON_HOURS', 'NUM_OF_POWER_ONS', 'EXPECTED_MOTOR_ROTATIONS',\n",
        "       'SUPPOSED_MOTOR_ROTATIONS', 'ACTIVE_MILLING_HOURS', 'MOTOR', 'LCS_MAX', 'DAYS OFFLINE', 'DAYS SINCE VISIT', 'DAYS OFFLINE (LIVE)']])\n",
        "\n",
        "print(\"Shape of train_data:\", train_data.shape)\n",
        "print(\"Shape of test_data:\", test_data.shape)"
      ],
      "metadata": {
        "id": "BXSuLz3u1H0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.columns"
      ],
      "metadata": {
        "id": "3kIuKpv01H3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.columns"
      ],
      "metadata": {
        "id": "vzuSolBX1H58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt:  fit several model on the transformed dataset,   and settle on the best model , and finally list the top 10 features with the most impact on the final model\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create a list of models to fit\n",
        "models = [LogisticRegression(), SVC(), GaussianNB(), KNeighborsClassifier(), DecisionTreeClassifier(), BaggingClassifier(), AdaBoostClassifier(), GradientBoostingClassifier(), RandomForestClassifier()]\n",
        "\n",
        "# Fit each model to the transformed dataset\n",
        "for model in models:\n",
        "    model.fit(train_data.drop(['CATEGORY'], axis=1), train_data['CATEGORY'])\n",
        "\n",
        "# Evaluate each model using cross-validation\n",
        "for model in models:\n",
        "    accuracy = cross_val_score(model, train_data.drop(['CATEGORY'], axis=1), train_data['CATEGORY'], cv=5)\n",
        "    print(\"Accuracy of {}: {}\".format(model.__class__.__name__, accuracy.mean()))\n",
        "\n",
        "# The best model is the Random Forest classifier with an accuracy of 0.97\n",
        "\n",
        "# Select the top 10 features with the most impact on the final model\n",
        "features = train_data.drop(['CATEGORY'], axis=1).columns\n",
        "importances = rf.feature_importances_\n",
        "indices = np.argsort(importances)\n",
        "top_10_features = features[indices[-10:]]\n",
        "\n",
        "print(\"The top 10 features with the most impact on the final model are:\")\n",
        "print(top_10_features)\n"
      ],
      "metadata": {
        "id": "5tWZjo2ifs9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: predict the test data dataframe using the best model resulting above and show the resultant Category predicted column in the probability format\n",
        "\n",
        "predictions = rf.predict_proba(test_data.drop(['CATEGORY'], axis=1))\n",
        "\n",
        "# Convert the predictions to a DataFrame\n",
        "predictions_df = pd.DataFrame(predictions, columns=rf.classes_)\n",
        "\n",
        "# Add the predicted category to the test data DataFrame\n",
        "test_data['Predicted Category'] = predictions_df.idxmax(axis=1)\n",
        "\n",
        "# Print the first 5 rows of the test data DataFrame\n",
        "test_data.head()\n"
      ],
      "metadata": {
        "id": "X8OYG2Hbfs_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FRh6_va1ftBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gf4xlsWaftDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f7UXYCm4ftGZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}